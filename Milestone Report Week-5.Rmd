---
title: "Milestone Report WEEK 5"
author: "Prahlad"
date: "23/06/2020"
output: html_document
---

```{r}
options(java.parameters = "-Xmx1024m")
library(RJDBC)
```


# Introduction
As part of the final Capstone Project we will practically follow the step by step approach as given below: 
1. Understanding the problem - Business case
2. Data acquisition - reading the data
3. Data cleaning - removing redundant info and preparing for analysis
3. Exploratory analysis - pre processing - NLP Corpus
4. Statistical modeling - n-gram models
5. Predictive modeling -  Based on n-gram 
6. Creative exploration - evaluating performance
7. Creating a data product - Shiny App
8. Creating a short slide deck pitching your product

This is an intermediate report that describes the data acquisition, cleaning and exploratory analysis. We will use the libraries as given below.

#List of Packages and Libraries
```{r, libraries}
library(stringi)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(htmlTable)
library(xtable)
library(knitr)
library(dplyr)
library(tm)
library(NLP)
library(ngram)
library(RWeka)
library(slam)
library(data.table)
library(rJava)
library(ggplot2)
library(R.utils)
library(dplyr)
library(parallel)
```

# Reading the Data
```{r}
con1 <- file("./final/en_US/en_US.twitter.txt", open = "rb")
twitter <- readLines(con1, skipNul = TRUE, encoding="UTF-8")
close(con1)

con2 <- file("./final/en_US/en_US.news.txt", open = "rb")
news <- readLines(con2, skipNul = TRUE, encoding="UTF-8")
close(con2)

con3 <- file("./final/en_US/en_US.blogs.txt", open = "rb")
blogs <- readLines(con3, skipNul = TRUE, encoding="UTF-8")
close(con3)
```

```{r}
list.files("./final")
```
The  unzipped data is stored in the directory -"final". We will discard the data of de_DE(German),  fi_FI(Finnish), ru_RU(Russian). 
Here we will use the English language data en_US(English). 
This data set has text from three different sources: 
1. Blogs - en_US.blogs.txt
2. News  - en_US.news.txt
3. Twitter - en_US.twitter.txt

# Basic statistical data summary of these 3 files
We will use function "readLines" to load the data in the R workspace.
Next we will conver to dataframe for further analysis
  
# Since the files are big we will use a "sample" Function to for partof the data (Try with 10% if doesn't work then keep going down)
#Function for taking a sample 
```{r, sampletext}
samplingText <- function(text_BNT, partof) {
  sampling <- sample(1:length(text_BNT), length(text_BNT)*partof)
  SampleText <- text_BNT[sampling]
  SampleText
}
```
# Three file namely blog, news and twittwer sampled with 1% samples. Bigger sample gives a memory saturation.  
```{r}
set.seed(20200623)
partof <- 1/100
sample_Blog<- samplingText(blogs, partof)
sample_Twitter <- samplingText(twitter, partof)
sample_News <- samplingText(news, partof)
```
# combine above sampled texts into one variable
```{r, combinedsamples}
sample_002 <- c(sample_Blog, sample_News, sample_Twitter)

```

# Save the sample_002 (sample text file) in the curret directory on local disc for future use
```{r, writesonlocaldisc}
writeLines(sample_002, "./samples_BNT/sample_002.txt")

```

```{r, }
stri_stats_general(sample_002) #Statistics of sample text
```
```{r}
summary( nchar(sample_002))
```
# Exploratory Data Analysis

**1. Cleaning the data** 
**1.1 Remove the unwanted punctuations, stopwords, unwanted numbers, spaces, convert to lowercase etc;.**

```{r}
 Space_r <- content_transformer(function(x, pattern) gsub(pattern, " ", x))# Function for removing unwanted
 clean_BNT <- function (clean_Text) {
  clean_Text <- tm_map(clean_Text, Space_r, "(f|ht)tp(s?)://(.*)[.][a-z]+") # Remove spaces 
  clean_Text <- tm_map(clean_Text, Space_r, "@[^\\s]+")
  clean_Text <- tm_map(clean_Text, content_transformer(tolower))
  clean_Text <- tm_map(clean_Text, stripWhitespace)
  clean_Text <- tm_map(clean_Text, removePunctuation)
  clean_Text <- tm_map(clean_Text, removeNumbers)
  clean_Text <- tm_map(clean_Text, PlainTextDocument)
  clean_Text
  }
```

# Creating a VectorCorpus
```{r}
sample_002 <- VCorpus(DirSource("./samples_BNT", encoding = "UTF-8"))
```

# Tokenizing the sampled and cleaned text 
```{r}
sample_002 <- clean_BNT(sample_002)

```

# Define function to make n-grams
```{r}
  tdm_Ngram <- function (clean_Text, n) {
  NgramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = n, max = n))}
  tdm_ngram <- TermDocumentMatrix(clean_Text, control = list(tokenizer = NgramTokenizer))
  tdm_ngram
}

```

# Define function to extract the N grams and sort
```{r}
ngram_sorted_df <- function (tdm_ngram) {
  tdm_ngram_matrix <- as.matrix(tdm_ngram)
  tdm_ngram_df <- as.data.frame(tdm_ngram_matrix)
  colnames(tdm_ngram_df) <- "Count"
  tdm_ngram_df <- tdm_ngram_df[order(-tdm_ngram_df$Count), , drop = FALSE]
  tdm_ngram_df
}
```

# Calculate N-Grams
```{r}
tdm_uni_gram <- tdm_Ngram(sample_002, 1)
tdm_bi_gram <- tdm_Ngram(sample_002, 2)
tdm_tri_gram <- tdm_Ngram(sample_002, 3)
tdm_quad_gram <- tdm_Ngram(sample_002, 4)
tdm_quant_gram <- tdm_Ngram(sample_002,5)

```


# Extract term-count tables from N-Grams and sort 
```{r}
tdm_uni_gram_df <- ngram_sorted_df(tdm_uni_gram)
tdm_bi_gram_df <- ngram_sorted_df(tdm_bi_gram)
tdm_tri_gram_df <- ngram_sorted_df(tdm_tri_gram)
tdm_quad_gram_df <- ngram_sorted_df(tdm_quad_gram)
tdm_quant_gram_df <- ngram_sorted_df(tdm_quant_gram)
```

# Save data frames into r-compressed files
#QuantGram data

```{r}
quantgram <- data.frame(rows=rownames(tdm_quant_gram_df),count=tdm_quant_gram_df$Count)
quantgram$rows <- as.character(quantgram$rows)
quantgram_split <- strsplit(as.character(quantgram$rows),split=" ")
quantgram <- transform(quantgram,first = sapply(quantgram_split,"[[",1),second = sapply(quantgram_split,"[[",2),third = sapply(quantgram_split,"[[",3), fourth = sapply(quantgram_split,"[[",4), fifth=sapply(quantgram_split,"[[",5))
quantgram <- data.frame(unigram = quantgram$first,bigram = quantgram$second, trigram = quantgram$third, quadgram = quantgram$fourth, quantgram=quantgram$fifth, freq = quantgram$count,stringsAsFactors=FALSE)
write.csv(quantgram[quantgram$freq > 1,],"./ShinyApp/quantgram.csv",row.names=F)
quantgram <- read.csv("./ShinyApp/quantgram.csv",stringsAsFactors = F)
saveRDS(quantgram,"./ShinyApp/quantgram.RData")

```

#QuadGram Data
```{r}
quadgram <- data.frame(rows=rownames(tdm_quad_gram_df),count=tdm_quad_gram_df$Count)
quadgram$rows <- as.character(quadgram$rows)
quadgram_split <- strsplit(as.character(quadgram$rows),split=" ")
quadgram <- transform(quadgram,first = sapply(quadgram_split,"[[",1),second = sapply(quadgram_split,"[[",2),third = sapply(quadgram_split,"[[",3), fourth = sapply(quadgram_split,"[[",4))
quadgram <- data.frame(unigram = quadgram$first,bigram = quadgram$second, trigram = quadgram$third, quadgram = quadgram$fourth, freq = quadgram$count,stringsAsFactors=FALSE)
write.csv(quadgram[quadgram$freq > 1,],"./ShinyApp/quadgram.csv",row.names=F)
quadgram <- read.csv("./ShinyApp/quadgram.csv",stringsAsFactors = F)
saveRDS(quadgram,"./ShinyApp/quadgram.RData")

```

# Trigram Data
```{r}
trigram <- data.frame(rows=rownames(tdm_tri_gram_df),count=tdm_tri_gram_df$Count)
trigram$rows <- as.character(trigram$rows)
trigram_split <- strsplit(as.character(trigram$rows),split=" ")
trigram <- transform(trigram,first = sapply(trigram_split,"[[",1),second = sapply(trigram_split,"[[",2),third = sapply(trigram_split,"[[",3))
trigram <- data.frame(unigram = trigram$first,bigram = trigram$second, trigram = trigram$third, freq = trigram$count,stringsAsFactors=FALSE)
write.csv(trigram[trigram$freq > 1,],"./ShinyApp/trigram.csv",row.names=F)
trigram <- read.csv("./ShinyApp/trigram.csv",stringsAsFactors = F)
saveRDS(trigram,"./ShinyApp/trigram.RData")
```

# Bigram data
```{r}
bigram <- data.frame(rows=rownames(tdm_bi_gram_df),count=tdm_bi_gram_df$Count)
bigram$rows <- as.character(bigram$rows)
bigram_split <- strsplit(as.character(bigram$rows),split=" ")
bigram <- transform(bigram,first = sapply(bigram_split,"[[",1),second = sapply(bigram_split,"[[",2))
bigram <- data.frame(unigram = bigram$first,bigram = bigram$second,freq = bigram$count,stringsAsFactors=FALSE)
write.csv(bigram[bigram$freq > 1,],"./ShinyApp/bigram.csv",row.names=F)
bigram <- read.csv("./ShinyApp/bigram.csv",stringsAsFactors = F)
saveRDS(bigram,"./ShinyApp/bigram.RData")
```



